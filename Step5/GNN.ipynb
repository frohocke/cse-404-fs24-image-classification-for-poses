{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch_geometric in c:\\users\\toman\\anaconda3\\lib\\site-packages (2.6.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: aiohttp in c:\\users\\toman\\anaconda3\\lib\\site-packages (from torch_geometric) (3.9.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\toman\\anaconda3\\lib\\site-packages (from torch_geometric) (2024.3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\toman\\anaconda3\\lib\\site-packages (from torch_geometric) (3.1.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\toman\\anaconda3\\lib\\site-packages (from torch_geometric) (1.26.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\toman\\anaconda3\\lib\\site-packages (from torch_geometric) (5.9.0)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\toman\\anaconda3\\lib\\site-packages (from torch_geometric) (3.0.9)\n",
      "Requirement already satisfied: requests in c:\\users\\toman\\anaconda3\\lib\\site-packages (from torch_geometric) (2.32.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\toman\\anaconda3\\lib\\site-packages (from torch_geometric) (4.66.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\toman\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\toman\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\toman\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\toman\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\toman\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (1.9.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\toman\\anaconda3\\lib\\site-packages (from jinja2->torch_geometric) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\toman\\anaconda3\\lib\\site-packages (from requests->torch_geometric) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\toman\\anaconda3\\lib\\site-packages (from requests->torch_geometric) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\toman\\anaconda3\\lib\\site-packages (from requests->torch_geometric) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\toman\\anaconda3\\lib\\site-packages (from requests->torch_geometric) (2024.7.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\toman\\anaconda3\\lib\\site-packages (from tqdm->torch_geometric) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActionRecognitionGNN(\n",
      "  (conv1): GCNConv(2, 64)\n",
      "  (conv2): GCNConv(64, 32)\n",
      "  (conv3): GCNConv(32, 15)\n",
      "  (classifier): Linear(in_features=15, out_features=15, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\toman\\anaconda3\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class PENNGraphDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir):\n",
    "        self.pose_annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        \n",
    "        # Define edges based on the connections between joints\n",
    "        self.edge_index = torch.tensor([\n",
    "            # Head to shoulders\n",
    "            [0, 1], [0, 2],           # Head to left and right shoulders\n",
    "\n",
    "            # Left arm\n",
    "            [1, 3], [3, 5],           # Left shoulder to left elbow, left elbow to left wrist\n",
    "\n",
    "            # Right arm\n",
    "            [2, 4], [4, 6],           # Right shoulder to right elbow, right elbow to right wrist\n",
    "\n",
    "            # Torso\n",
    "            [1, 7], [2, 8],           # Left shoulder to left hip, right shoulder to right hip\n",
    "\n",
    "            # Left leg\n",
    "            [7, 9], [9, 11],          # Left hip to left knee, left knee to left ankle\n",
    "\n",
    "            # Right leg\n",
    "            [8, 10], [10, 12],        # Right hip to right knee, right knee to right ankle\n",
    "\n",
    "            # Mid-torso connection\n",
    "            [7, 8]                    # Left hip to right hip (connecting the lower torso)\n",
    "        ], dtype=torch.long).t().contiguous()  # .t() transposes the array\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pose_annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.pose_annotations.iloc[idx]\n",
    "        \n",
    "        # Extract node features (2D coordinates and visibility) from the row\n",
    "        features = [[float(coord) for coord in ast.literal_eval(joint)[:2]] for joint in row[:-1].values]\n",
    "        features = torch.tensor(features, dtype=torch.float)\n",
    "\n",
    "        # Create the graph data object\n",
    "        data = Data(x=features, edge_index=self.edge_index, y=torch.tensor(row[-1], dtype=torch.long))\n",
    "        return data\n",
    "\n",
    "# Initialize dataset\n",
    "train_data = PENNGraphDataset(\n",
    "    csv_file=os.path.join(\"..\", \"Penn_Action\", \"TrainTable.csv\"),\n",
    "    root_dir=os.path.join(\"..\", \"Penn_Action\")\n",
    ")\n",
    "test_data = PENNGraphDataset(\n",
    "    csv_file=os.path.join(\"..\", \"Penn_Action\", \"TestTable.csv\"),\n",
    "    root_dir=os.path.join(\"..\", \"Penn_Action\")\n",
    ")\n",
    "\n",
    "batch_size=30\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# Step 3: Define the GNN Model\n",
    "class ActionRecognitionGNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActionRecognitionGNN, self).__init__()\n",
    "        self.conv1 = GCNConv(2, 64)  # Input features (2 coordinates) to 64 hidden units\n",
    "        self.conv2 = GCNConv(64, 32)  # 64 hidden units to 32\n",
    "        self.conv3 = GCNConv(32, 15)  # 32 hidden units to output (number of classes)\n",
    "\n",
    "        # Define the classifier layer for final output\n",
    "        self.classifier = nn.Linear(15, 15)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Apply GCN layers\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        \n",
    "        # Pooling across nodes in each graph, keeping batch dimension intact\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # Classify each graph in the batch\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Instantiate model and move to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = ActionRecognitionGNN().to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cross-entropy loss as the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Define a pytorch optimizer using stochastic gradient descent (SGD)\n",
    "optimizer = torch.optim.SGD(model.parameters(),learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer): \n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    losses = []\n",
    "    for batch, data in enumerate(dataloader):\n",
    "        data = data.to(device)  # Move batch to device\n",
    "        X, edge_index, batch_tensor = data.x, data.edge_index, data.batch  # Unpack features, edges, and batch\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X, edge_index, batch_tensor)  # Pass all needed inputs to the model\n",
    "\n",
    "        # Compute training loss\n",
    "        loss = loss_fn(pred, data.y)  # data.y is the target tensor\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log the loss\n",
    "        losses.append(loss.item())\n",
    "        if batch % 100 == 0:\n",
    "            print(f\"loss: {loss.item():>7f}  [{batch * len(X):>5d}/{size:>5d}]\")\n",
    "\n",
    "    return np.mean(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    model.eval()  # Set the model to eval mode\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():  # no_grad mode doesn't compute gradients\n",
    "        for batch in dataloader:\n",
    "            # Extract batch data\n",
    "            X, y = batch.x.to(device), batch.y.to(device)\n",
    "            edge_index = batch.edge_index.to(device)\n",
    "            batch_idx = batch.batch.to(device)\n",
    "\n",
    "            # Pass X, edge_index, and batch_idx to the model\n",
    "            pred = model(X, edge_index, batch_idx)\n",
    "            \n",
    "            # Compute test loss and accuracy\n",
    "            test_loss += loss_fn(pred, y).item()  # compute the test loss\n",
    "            correct += (pred.argmax(dim=1) == y).sum().item()  # count correct predictions\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "# for plotting the training loss\n",
    "history = {'losses': [], 'accuracies': []}\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    history['losses'].append(train(train_dataloader, model, loss_fn, optimizer))\n",
    "    history['accuracies'].append(test(test_dataloader, model, loss_fn))\n",
    "    plt.clf()\n",
    "    fig1 = plt.figure()\n",
    "    plt.plot(history['losses'], 'r-', lw=2, label='loss')\n",
    "    plt.legend()\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "    plt.clf()\n",
    "    fig2 = plt.figure()\n",
    "    plt.plot(history['accuracies'], 'b-', lw=1, label='accuracy')\n",
    "    plt.legend()\n",
    "#     display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
